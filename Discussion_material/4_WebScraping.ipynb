{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8bb859f",
   "metadata": {},
   "source": [
    "# Zillow Webscraping + Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284aec53",
   "metadata": {},
   "source": [
    "## 1. Webscraping:\n",
    "\n",
    "In this part of the code, I'll show you how to extract a data frame with the houses for sale in Champaign from www.zillow.com. This code only extract the first page of the static webpage. Therefore, the resulting data frame will contain only 40 rows (houses/apts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6360e3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd8c487",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_headers = {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'en-US,en;q=0.8',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'\n",
    "}\n",
    "\n",
    "with requests.Session() as s:\n",
    "    city = 'champaign/' \n",
    "    url = 'https://www.zillow.com/homes/for_sale/'+city    \n",
    "    r = s.get(url, headers=req_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81ca631",
   "metadata": {},
   "outputs": [],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663e7f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144ab2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc536cb",
   "metadata": {},
   "source": [
    "## Create DataFrame based on classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3fd01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "df['price'] = soup.find_all(class_='list-card-price')\n",
    "df['address'] = soup.find_all(class_='list-card-addr')\n",
    "df['beds'] = soup.find_all(\"ul\", class_=\"list-card-details\")\n",
    "\n",
    "#df['link']  = list(soup.find_all(class_= 'list-card-link'))\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c18a8ba",
   "metadata": {},
   "source": [
    "## What makes up the contents of the dataframe?? Are they a list or what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cc978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82dd5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['price'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e22f0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'][0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d89cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['price'][0].get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294b252b",
   "metadata": {},
   "source": [
    "Now that's better... We know how to work with strings! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042c254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRICES\n",
    "df['price']=[x.get_text() for x in df['price']]\n",
    "df['price'] = df['price'].str.replace(r'\\D', '')  # \\D is regex for non-digit. \n",
    "df[['price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0d5123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADDRESSES:\n",
    "df['address']=[x.get_text() for x in df['address']]\n",
    "df[['address']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea9802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['beds']=[x.get_text() for x in df['beds']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a223e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEDS - BATHS\n",
    "df['beds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f4ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['beds','baths']] = df.beds.str.split(\" bds\",expand=True)\n",
    "# documentation on split with pandas series: https://pandas.pydata.org/docs/reference/api/pandas.Series.str.split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41d0a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['beds','baths']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1901bd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQFeet\n",
    "df[['baths','sq_feet']] = df.baths.str.split(\" ba\",expand=True)\n",
    "df[['baths','sq_feet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab4d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPE \n",
    "df[['sq_feet','type']] = df.sq_feet.str.split(\" sqft- \",expand=True)\n",
    "df[['sq_feet','type']]\n",
    "\n",
    "# There are alternative ways to get rid of html tags. Here is the manual way: \n",
    "# df['beds'] = df['beds'].astype('str')\n",
    "# df['beds'] = df['beds'].replace('<ul class=\"list-card-details\"><li class=\"\">','', regex=True)\n",
    "# df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->','',regex=True)\n",
    "# df['beds'] = df['beds'].replace('</abbr></li><li class=\"\">','-',regex=True)\n",
    "# df['beds'] = df['beds'].replace('<abbr class=\"list-card-label\"> <!-- -->','',regex=True)\n",
    "# df['beds'] = df['beds'].replace('</abbr></li><li class=\"\">','-',regex=True)\n",
    "# df['beds'] = df['beds'].replace('</abbr></li><li class=\"list-card-statusText\">','',regex=True)\n",
    "# df['beds'] = df['beds'].replace('</li></ul>','',regex=True)\n",
    "# df[['beds','baths','sq_feet','type','none1']] = df.beds.str.split(\"-\",expand=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c0686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c2c269",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'] = df['price'].astype('int')\n",
    "df['beds'] = df['beds'].astype('float')\n",
    "df['baths'] = df['baths'].astype('float')\n",
    "df['sq_feet'] = df['sq_feet'].str.replace(r'\\D', '').astype('float')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2566dc28",
   "metadata": {},
   "source": [
    "## Obtaining the link of the house/apt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13d4e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = soup.find_all(class_= 'list-card-link')\n",
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2421c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"article\")[0].find('a',class_=\"list-card-link\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa6027d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup.find_all(\"article\")[0].find('a',class_=\"list-card-link\").get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c71378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty url list\n",
    "urls = []\n",
    "\n",
    "#loop through url, pull the href and strip out the address tag\n",
    "for link in range(len(soup.find_all(\"article\"))-1):\n",
    "    url = soup.find_all(\"article\")[link].find('a',class_=\"list-card-link\").get('href')\n",
    "    urls.append(url)\n",
    "\n",
    "df['urls'] = urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becbca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c9ddb2",
   "metadata": {},
   "source": [
    "## Webscraping each house/apt link.\n",
    "\n",
    "From each specific link, we will get the descriptions and latitude/longitude of each house/apt for sale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe45d07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use same req_header as before to avoid captchas from Zillow...\n",
    "req_headers = {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'en-US,en;q=0.8',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156132d0",
   "metadata": {},
   "source": [
    "### Let's just start with one house first to get the feel for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb27258",
   "metadata": {},
   "outputs": [],
   "source": [
    "link=df['urls'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357e41cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with requests.Session() as s:\n",
    "    url = link\n",
    "    r2 = s.get(url, headers=req_headers)\n",
    "r2.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba975a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d88e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2 = BeautifulSoup(r2.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971075f8",
   "metadata": {},
   "source": [
    "#### Getting lat/lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aad10d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting latitude and longitude:     \n",
    "latlon = soup.find('script', {'type':'application/ld+json'})\n",
    "latlon = json.loads(latlon.contents[0])\n",
    "latitude = latlon['geo']['latitude']\n",
    "longitude = latlon['geo']['longitude']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59366f03",
   "metadata": {},
   "source": [
    "#### Getting Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e088369",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "for element in soup2.find_all(class_=True):\n",
    "    classes.extend(element[\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede01933",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bfa1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches= [match for match in classes if \"Text\" in match]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbf334f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(matches) #Text-c11n-8-18-0__aiai24-0 sc-qPwPv cZodDt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbc5b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=soup2.find_all(class_='Text-c11n-8-62-5__sc-aiai24-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cf3d8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82b0da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=soup2.find_all(class_='Text-c11n-8-62-5__sc-aiai24-0 sc-AjmGg kZKvMY dyvPlo')\n",
    "description1 = [d.text for d in a]\n",
    "description1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfa2acd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a=soup2.find_all(class_='Text-c11n-8-62-5__sc-aiai24-0 sc-AjmGg kZKvMY kYGWTz')\n",
    "description2 = [d.text for d in a]\n",
    "description2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4ca462",
   "metadata": {},
   "source": [
    "## Using a loop to evaluate all the links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318bb0b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['urls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef2717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "descrip = []\n",
    "descrip2 = []\n",
    "lat = []\n",
    "lon = []\n",
    "\n",
    "for link in df['urls']:\n",
    "    r = s.get(link, headers=req_headers)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    # Gettting description\n",
    "    description= soup.find_all(class_='Text-c11n-8-62-5__sc-aiai24-0 sc-AjmGg kZKvMY dyvPlo')\n",
    "    description = [d.text for d in description]\n",
    "    descrip.append(description)\n",
    "    \n",
    "    # Gettting description - version 2 \n",
    "    description2= soup.find_all(class_='Text-c11n-8-62-5__sc-aiai24-0 sc-AjmGg kZKvMY kYGWTz')\n",
    "    description2 = [d.text for d in description2]\n",
    "    descrip2.append(description2)\n",
    "    \n",
    "    \n",
    "    # Getting latitude and longitude:     \n",
    "    latlon = soup.find('script', {'type':'application/ld+json'})\n",
    "    latlon = json.loads(latlon.contents[0])\n",
    "    latitude = latlon['geo']['latitude']\n",
    "    longitude = latlon['geo']['longitude']\n",
    "    \n",
    "    lat.append(latitude)\n",
    "    lon.append(longitude)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6931b48e",
   "metadata": {},
   "source": [
    "### Addind the new columns to our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090df37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lat'] = lat\n",
    "df['lon'] = lon\n",
    "# description 1\n",
    "df['descrip'] = descrip\n",
    "df['descrip'] = df['descrip'].astype('str')\n",
    "df['descrip']  = df['descrip'].replace('\\[', '', regex=True)\n",
    "df['descrip']  = df['descrip'].replace('\\]', '', regex=True)\n",
    "\n",
    "# description 2\n",
    "df['descrip2'] = descrip2\n",
    "df['descrip2'] = df['descrip2'].astype('str')\n",
    "df['descrip2']  = df['descrip2'].replace('\\[', '', regex=True)\n",
    "df['descrip2']  = df['descrip2'].replace('\\]', '', regex=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5817f26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "usedescrip=[len(x)>0 for x in df['descrip']]\n",
    "usedescrip2 = [len(x)>0 for x in df['descrip2']]\n",
    "df.loc[usedescrip,'description'] = df.loc[usedescrip,'descrip']\n",
    "df.loc[usedescrip2,'description'] = df.loc[usedescrip2,'descrip2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a07483",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f461a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3082f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['price', 'address', 'beds', 'baths', 'sq_feet', 'type', 'urls', 'lat','lon', 'description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe4fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15c1f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you want to save the resulting dataframe: \n",
    "#df.to_csv(r'ZillowWebscrap_Champaign_page1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f23d19",
   "metadata": {},
   "source": [
    "## 2. Text Analysis\n",
    "\n",
    "We'll do text analysis over the description of each listing. However, we will use a file that contains all the 6 pages from Zillow (Instead of only one as we doid above). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b75c9e",
   "metadata": {},
   "source": [
    "### Remove punctuation and split words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c71a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ZillowWebscrap_Champaign.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528af07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb17f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6a687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lower case variable, and remove \\n (new lines) character in case we have: \n",
    "df['desctiption_lower'] =  df['description'].str.lower().str.replace(\"\\n\",\"\")\n",
    "# Remove punctuation and list of characters that we need to remove \n",
    "remv_punc = str.maketrans('','',string.punctuation + '“' +\"‘\"+'”')\n",
    "\n",
    "df['description_clean'] =  df['desctiption_lower'].str.translate(remv_punc)\n",
    "# Use of regular expressoin to remove digits: \n",
    "df['description_clean'] = [re.sub(\"\\d+\", \"\", x) for x in df['description_clean']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262a4f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [x.split(\" \") for x in df['description_clean']]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca74da4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to download the stopwords first. I don't need to do that again.\n",
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f03a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628690b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dccde2",
   "metadata": {},
   "source": [
    "## Must remove punctuaction from stop words list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4644b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_words=stopwords.words('english')+ [\" \", ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1620a191",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dem_words a string instead of a list for the next step\n",
    "dem_wordz=' '.join(dem_words)\n",
    "dem_wordz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd366d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove puncuation from stop words string\n",
    "remv_punc = str.maketrans('','',string.punctuation)\n",
    "dem_words_no_punc=dem_wordz.translate(remv_punc).lower()\n",
    "dem_words_no_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa6bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dem_words_no_punc back to a list \n",
    "dem_words_final=dem_words_no_punc.split(\" \")\n",
    "dem_words_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523865af",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list = [pd.Series(x).value_counts() for x in words]\n",
    "word_df = pd.concat(words_list,axis=1).fillna(0).T\n",
    "word_df;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87629f2a",
   "metadata": {},
   "source": [
    "### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3800eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords:\n",
    "words_nsw = word_df.loc[:,~word_df.T.index.isin(dem_words_final)]\n",
    "words_nsw.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d066fffe",
   "metadata": {},
   "source": [
    "### Words counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfad6b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_nsw.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97785e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_nsw.sum().sort_values().tail(20).plot(kind='barh',figsize=(7,5));\n",
    "plt.xlabel(\"Number of times\");\n",
    "plt.title(\"Frequently used words in Zillow listings \\n Houses for rent. Champaign\",fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c90dd6",
   "metadata": {},
   "source": [
    "### An application of Term Frequency - Inverse Document Frequency\n",
    "\n",
    "In this case, a document will be a row of our original data. (i.e., one listing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea30392",
   "metadata": {},
   "source": [
    "**Term Frequency - Inverse Document Frequency (TF-IDF)** \n",
    "\n",
    "Term frequency: how often does a word appear in a document?\n",
    "\n",
    "Document frequency: How many documents contain this word?\n",
    "\n",
    "We divide term frequency by the total number of documents that have that word: $TF DF = TF/DF$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1887ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_nswT = words_nsw.T\n",
    "words_nswT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eed2f5c",
   "metadata": {},
   "source": [
    "Term frequency: How many times a word shows up in a document (and here a document is each housing description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5233ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_calc(column):\n",
    "    return column/column.sum()\n",
    "\n",
    "tf = words_nswT.apply(tf_calc,axis=1)\n",
    "tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c785826",
   "metadata": {},
   "source": [
    "Document Frequency: How many documents contain this word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8145a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dcbcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_nswT.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c07dc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Now calculate IDF:\n",
    "inv_doc_freq = np.log(tf.shape[1]/(words_nswT!=0).sum(axis=1))\n",
    "inv_doc_freq.sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7173a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, using vectorization method:\n",
    "\n",
    "idf_mat= np.repeat(np.array(inv_doc_freq)[:,np.newaxis],\\\n",
    "                   tf.shape[1],\\\n",
    "                   axis=1)\n",
    "\n",
    "tf_idf = tf*idf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa549c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371c98a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking one listing:\n",
    "listing_no= 10\n",
    "tf_idf[listing_no][tf_idf[listing_no]<5.9].sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f9671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking words with highest values of TD-DF (overall)\n",
    "tf_idf['mean'] = tf_idf.mean(axis=1)\n",
    "tf_idf['mean'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9f5c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf['mean'].sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76855ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_high = tf_idf['mean'].sort_values(ascending=False).head(10)\n",
    "words_high = list(words_high.index)\n",
    "words_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11be259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idfT = tf_idf.T\n",
    "tf_idfT_sub = tf_idfT[words_high]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bdc661",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[words_high] = tf_idfT_sub\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900561ec",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b106c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34a377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f30bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = [sid.polarity_scores(x)['compound'] for x in df['description_clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d15c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aed856",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87490aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "senti=df[['sentiment','description_clean','address']]\n",
    "# df.sort_values(['sentiment','description'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05774a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e50c99e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "senti.sort_values(by='sentiment',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d914feea",
   "metadata": {},
   "outputs": [],
   "source": [
    "senti.sort_values(by='sentiment',ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5d8fb6",
   "metadata": {},
   "source": [
    "### Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db2d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import econtools.metrics as mt\n",
    "from econtools.metrics import reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c295e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(words_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48eaf4e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3431136d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reg(df,\"price\",[\"beds\",\"sentiment\",'baths'], addcons=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
